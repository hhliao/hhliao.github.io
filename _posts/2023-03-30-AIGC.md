# CLIP
**numpt-like pseudocode for the core implementation of CLIP**

    # image_encoder - ResNet or Vision Transformer
    # text_encoder  - CBOW or Text Transformer
    # I[n, h, w, c] - minibatch of aligned images
    # T[n, l]       - minibatch of aligned texts
    # W_i[d_i, d_e] - learned proj of image to embed
    # W_t[d-t, d-e] - learned proj of text to embed
    # t             - learned temperature parameter
    
    # extract feature representations of each modality
    I_f = image_encoder(I) # [n, d_i]
    T_f = text_encoder(T)  # [n, d_t]
    
    # join moltimodal embedding [n, d_e]
    I_e = l2_normalize(np.dot(I_f, W_i), axis=1)
    T_e = l2_normalize(np.dot(T_f, W_t), axis=1)
    
    # scaled paiwise cosine similarities [n, n]
    logits = np.dot(I_e, T_e.T) * np.exp(t)
    
    # symmetric loss function
    labels = np.arange(n)
    loss_i = cross_entropy_loss(logits, labels, axis=0)
    loss_t = cross_entropy_loss(logits, labels, axis=1)
    loss   = (loss_i + loss_t) / 2
