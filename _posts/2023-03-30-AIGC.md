# learning AIGC 

## CLIP
Radford A, Kim J W, Hallacy C, et al. Learning transferable visual models from natural language supervision[C]//International conference on machine learning. PMLR, 2021: 8748-8763.

**numpy-like pseudocode for the core implementation of CLIP**

    # image_encoder - ResNet or Vision Transformer
    # text_encoder  - CBOW or Text Transformer
    # I[n, h, w, c] - minibatch of aligned images
    # T[n, l]       - minibatch of aligned texts
    # W_i[d_i, d_e] - learned proj of image to embed
    # W_t[d-t, d_e] - learned proj of text to embed
    # t             - learned temperature parameter
    
    # extract feature representations of each modality
    I_f = image_encoder(I) # [n, d_i]
    T_f = text_encoder(T)  # [n, d_t]
    
    # join moltimodal embedding [n, d_e]
    I_e = l2_normalize(np.dot(I_f, W_i), axis=1)
    T_e = l2_normalize(np.dot(T_f, W_t), axis=1)
    
    # scaled paiwise cosine similarities [n, n]
    logits = np.dot(I_e, T_e.T) * np.exp(t)
    
    # symmetric loss function
    labels = np.arange(n)
    loss_i = cross_entropy_loss(logits, labels, axis=0)
    loss_t = cross_entropy_loss(logits, labels, axis=1)
    loss   = (loss_i + loss_t) / 2

## 数字人相关
数字人的两个重要步骤：
- Audio Generation (Text To Speech)
- Lip Sync ( Sync audio with Video )

### Audio Generation
 语音生成工具有：[tacotron project](https://google.github.io/tacotron/), [Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning), 和 [FastSpeech2](https://github.com/ming024/FastSpeech2).
 
### Lip Sync ( Sync audio with Video )
 这个步骤之前使用的是基于[GAN](https://learnopencv.com/introduction-to-generative-adversarial-networks/)的方法，但当前SOTA方法是 [wav2lip](http://bhaasha.iiit.ac.in/lipsync/).  
 本文来源[leanopencv](https://learnopencv.com/ai-behind-the-diwali-2021-not-just-a-cadbury-ad/)
