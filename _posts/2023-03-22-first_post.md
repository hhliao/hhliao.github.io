# LLMs 
Some of the most popular frameworks for model parallelism are
- FasterTransformer by NVIDIA for inference
- DeepSpeed for both training and inference
- Megatron-LM for training
- JAX for training and inference
- PyTorch 2.0


## NVIDIA Spring GTC 2023

### Understanding Transformers:

- [Understanding Attention Mechanism in Transformer Neural Networks](https://learnopencv.com/attention-mechanism-in-transformer-neural-networks/)
- [The Future of Image Recognition is Here: PyTorch Vision Transformers](https://learnopencv.com/the-future-of-image-recognition-is-here-pytorch-vision-transformer/)

Some more about inference see [NVIDIA Spring GTC 2023 Day 1: Welcome to the future!](https://learnopencv.com/nvidia-spring-gtc-2023-day-1-highlights-welcome-to-the-future/), part **Efficient Inference of Transformer Models**.

### Some advice form researcher (Amulya, Neena and Tomas) from Nividia:

1. Starting early helps.
1. You donâ€™t have to specialize too early. Experimentation, failure and success are part of the process.
1. Seek out challenging courses to build your foundations.
1. Always keep an eye on funding opportunities for projects and internships etc.
1. Expand your field of view and look for opportunities of cross disciplinary research.
1. Network extensively by attending technical conferences such as GTC ðŸ™‚
1. Another great way to network is by joining professional societies (they typically have discounts for student memberships).
